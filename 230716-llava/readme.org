#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:{} arch:headline
#+options: author:t broken-links:mark c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+options: timestamp:t title:t toc:1 todo:t |:t
#+title: Multimodal Large Language Model (MLLM)
#+date: <2023-07-16 Sun>
#+author: Nasy
#+email: nasyxx@gmail.com
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 29.0.50 (Org mode 9.5.5)
#+cite_export: biblatex
#+columns: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)

#+setupfile: https://raw.githubusercontent.com/uta-smile/beamer-theme/master/setup.org

#+latex_header_extra: \usepackage{minted}
#+latex_header_extra: \setbeamerfont{caption}{size=\scriptsize}
#+latex_header: \AtBeginSubsection[]{\begin{frame}<beamer>\frametitle{Section}\tableofcontents[currentsection,currentsubsection]\end{frame}}
#+latex_header: \synctex=1

#+latex: \setcounter{tocdepth}{2}

* Introduction

*** Introduction

+ What is Multimodal Large Language Model (MLLM)?
  - LLM-based model with the ability to receive and reason with multimodal information.
+ Future?
  - MLLM is more in line with the way humans perceive the world.
  - MLLM offers a more user-friendly interface.
+ Examples
  - GPT4
  - LLaVA
  - MiniGPT-4
  - ...

*** Benchmark

MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models[cite/ft/f:@fuMMEComprehensiveEvaluation2023]

#+attr_latex: :height 6cm
[[./p1.png]]

*** Overview

+ *Multimodal Instruction Tuning (M-IT)*
+ Multimodal In-Context Learning (M-ICL),
+ Multimodal Chain-of-Thought (M-CoT),
+ LLM-Aided Visual Reasoning (LAVR)

* Multimodal Instruction Tuning (M-IT)

*** Multimodal Instruction Tuning (M-IT)

+ Dataset:
  - Existing benchmark datasets
  - Self-instruction
+ Model:
  - Align foreign embeddings to the LLMs
  - Resort to expert models to translate foreign modalities into natural languages that LLMs can ingest
+ Fine-tuning:
  - LoRA

*** Multimodal Instruction Tuning (M-IT)

+ Works:
  - MiniGPT-4[cite/ft/f:@zhuMiniGPT4EnhancingVisionLanguage2023]
  - Visual Instruction Tuning (LLaVA) [cite/ft/f:@liuVisualInstructionTuning2023]
  - mPLUG-Owl[cite/ft/f:@yeMPLUGOwlModularizationEmpowers2023]

* MiniGPT-4

*** MiniGPT-4

#+attr_latex: :height 8cm
[[./p2.png]]

*** MiniGPT-4 methods

+ Vicuna (LLaMA)
+ BLIP-2 (ViT backbone with pre-trained Q-Former)
+ Linear projection to bridge the gap
+ Two stages
  - Pretraining the model on a large collection of aligned image-text pairs to acquire visionlanguage knowledge.
  - Fine-tune the pretrained model with a smaller but high-quality image-text dataset with a designed conversational template to enhance the modelâ€™s generation reliability and usability.

*** MiniGPT-4 First stage

+ Train
  - Conceptual Caption, SBU, and LAION
  - 20,000 training steps with a batch size of 256, covering approximately 5 million image-text pairs.
+ Issues
  - Generating repetitive words or sentences, fragmented sentences, or irrelevant content.

*** MiniGPT-4 First stage alignment

+ Two step
  - Align the image-text pairs generation (Template)
    * ~###Human: <Img><ImageFeature></Img> Describe this image in detail. Give as many details as possible. Say everything you see. ###Assistant:~
  - Data post processing (ChatGPT, fix the generated text)

*** MiniGPT-4 Final fine-tuning

+ Finetune the pretrained model with the curated high-quality image-text pairs.

+ ~###Human: <Img><ImageFeature></Img> <Instruction> ###Assistant:~

*** LLaVA

[[./p3.png]]

* Reference

*** Reference
:PROPERTIES:
:BEAMER_opt: allowframebreaks
:END:

#+print_bibliography:
